---
description: Interface for implementing AI providers to send prompts and receive responses including options and system prompt.
labels: ['interface', 'ai', 'provider', 'utility']
---

### Basic Text Generation

This example demonstrates a basic implementation of an AiProvider for generating text responses from a simple prompt. It shows how a concrete provider would fulfill the `AiProvider` interface by handling a request and returning a text response.

```typescript
// A simple mock AI provider for demonstration purposes
class MockTextAiProvider implements AiProvider {
  name = 'MockTextGenerator';
  async request(req: AiModelRequest): Promise<AiModelResponse> {
    console.log(`MockTextAiProvider received prompt: ${req.prompt}`);
    // Simulate an AI response
    const responseText = `Mock response to: "${req.prompt}". This is a generated text.`;
    return { text: responseText };
  }
}
// Usage of the mock provider
const textProvider = new MockTextAiProvider();
async function demonstrateBasicTextGeneration() {
  const request: AiModelRequest = {
    prompt: 'Tell me a short story about a brave knight.',
  };
  const response = await textProvider.request(request);
  console.log('AI Response:', response.text);
}
// To run this example:
// demonstrateBasicTextGeneration();
```

### Advanced Prompting with System Instructions

This example shows how to use system instructions and additional options to guide the AI model behavior for more specific outputs. An `AiProvider` implementation can interpret these parameters to tailor the AI model's response.

```typescript
// A mock AI provider that acknowledges system prompts and options
class AdvancedMockAiProvider implements AiProvider {
  name = 'AdvancedMockAI';
  async request(req: AiModelRequest): Promise<AiModelResponse> {
    console.log(`AdvancedMockAiProvider received request for model: ${req.model || 'default'}`);
    if (req.systemPrompt) {
      console.log(`System Prompt applied: ${req.systemPrompt}`);
    }
    if (req.options) {
      console.log('Options received:', req.options);
    }
    // Simulate a more context-aware response
    const generatedText = `Acknowledging your prompt: "${req.prompt}". The system instructions and options were noted for a more precise generation.`;
    return { text: generatedText };
  }
}
// Usage of the advanced mock provider
const advancedProvider = new AdvancedMockAiProvider();
async function demonstrateAdvancedPrompting() {
  const request: AiModelRequest = {
    prompt: 'Describe a futuristic city in detail.',
    systemPrompt: 'The response should be concise and focus on architecture.',
    options: {
      temperature: 0.7,
      maxTokens: 100,
    },
    model: 'gpt-4-turbo',
  };
  const response = await advancedProvider.request(request);
  console.log('Advanced AI Response:', response.text);
}
// To run this example:
// demonstrateAdvancedPrompting();
```

### Handling Streamed Responses

This example illustrates an AiProvider implementation that simulates a streaming response, providing chunks of text asynchronously. This is useful for user interfaces that want to display AI responses as they are generated.

```typescript
// A mock AI provider that simulates streaming
class StreamingMockAiProvider implements AiProvider {
  name = 'StreamingAI';
  async request(req: AiModelRequest): Promise<AiModelResponse> {
    if (!req.stream) {
      // If no stream callback is provided, return a complete response
      return { text: 'This is a complete non-streaming response.' };
    }
    console.log(`StreamingMockAiProvider received prompt for streaming: ${req.prompt}`);
    const fullResponse = `Simulated streaming response for: "${req.prompt}". Here is the first part... then the second part... and finally, the last part of the response.`;
    const chunks = fullResponse.match(/.{1,20}/g) || [fullResponse]; // Split into small chunks
    for (let i = 0; i < chunks.length; i++) {
      await new Promise(resolve => setTimeout(resolve, 50 * (i + 1))); // Simulate delay
      req.stream(chunks[i]);
    }
    return { text: fullResponse, metadata: { streamed: true } };
  }
}
// Usage of the streaming mock provider
const streamingProvider = new StreamingMockAiProvider();
async function demonstrateStreaming() {
  let receivedStreamedText = '';
  const request: AiModelRequest = {
    prompt: 'Explain quantum physics simply.',
    stream: (chunk) => {
      receivedStreamedText += chunk;
      console.log('Stream chunk received:', chunk);
    },
  };
  const finalResponse = await streamingProvider.request(request);
  console.log('Final streamed response text:', finalResponse.text);
  console.log('Accumulated streamed text:', receivedStreamedText);
}
// To run this example:
// demonstrateStreaming();